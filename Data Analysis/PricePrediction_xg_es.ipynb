{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "508e6abf-317e-46ad-b63a-865c2eeaf4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import findspark\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder #, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import xgboost as xgb\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error, r2_score\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, explode, sequence, to_date, lag\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler , StandardScaler\n",
    "from sklearn.svm import OneClassSVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9efe043d-a7bc-4aa0-b796-6c33778dd8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aea7921-6b24-464b-802e-1e0d38603f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jdk-11'\n",
    "os.environ['HADOOP_HOME'] = r'C:\\Program Files\\hadoop-2.7.1'\n",
    "os.environ['SPARK_HOME'] = r'C:\\Program Files\\Anaconda\\envs\\newenv\\Lib\\site-packages\\pyspark'\n",
    "os.environ['PYSPARK_PYTHON'] = r'C:\\Program Files\\Anaconda\\envs\\newenv\\python.exe'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = r'C:\\Program Files\\Anaconda\\envs\\newenv\\python.exe'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68e04757-752a-4752-bccf-36846f00aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!echo %JAVA_HOME%\n",
    "#!echo %HADOOP_HOME%\n",
    "#!echo %PATH%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c18062a0-04db-433b-be9f-2b371fe47e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the existing Spark session\n",
    "if 'spark' in locals():\n",
    "    spark.stop()\n",
    "spark = SparkSession.builder.appName(\"PricePredictionModel\").config(\"spark.driver.memory\", \"12g\").config(\"spark.executor.memory\", \"12g\").config(\"spark.driver.maxResultSize\", \"12g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "610b4b2a-142b-49f1-b0e3-9676e6ce8ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the Australian Grocery Dataset\n",
    "grocery_df = spark.read.csv(\"AustralianDataset/Australia_Grocery_2022Sep.csv\", header=True, inferSchema=True)\n",
    "grocery_df = grocery_df.withColumnRenamed(\"Package_price\", \"Package_price_grocery\")\n",
    "grocery_df = grocery_df.withColumnRenamed(\"RunDate\", \"RunDate_grocery\")\n",
    "\n",
    "# Load the Synthetic Dataset\n",
    "synthetic_df = spark.read.csv(\"AustralianDataset/synthethic_time_point.csv\", header=True, inferSchema=True)\n",
    "synthetic_df = synthetic_df.withColumnRenamed(\"Package_price\", \"Package_price_synthetic\")\n",
    "synthetic_df = synthetic_df.withColumnRenamed(\"RunDate\", \"RunDate_synthetic\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54a972f9-1ecb-41de-a6fc-cf6f2c7bf398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the datasets on SKU\n",
    "combined_df = grocery_df.join(synthetic_df, on=[\"Sku\", \"city\"], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f63a8d89-118a-44e9-ae42-f8522d43070f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+-----------+--------------------+------------+-------------+------------------+--------------+------------+------------+----------+--------+------------+--------------------+----------------+----------+---------------+-----+--------+--------+-------------+-------------------+\n",
      "|     Sku|      city| index|Postal_code|            Category|Sub_category|Product_Group|      Product_Name|Price_per_unit|package_size|is_estimated|is_special|in_stock|Retail_price|         Product_Url|           Brand|unit_price|unit_price_unit|state|     tid|     _c0|RunDate_final|Package_price_final|\n",
      "+--------+----------+------+-----------+--------------------+------------+-------------+------------------+--------------+------------+------------+----------+--------+------------+--------------------+----------------+----------+---------------+-----+--------+--------+-------------+-------------------+\n",
      "|1010087P|BIGGS FLAT|192519|       5153|Dairy, eggs & fridge|      Cheese| Block cheese|Colby Cheese Block|$22.80 per 1Kg|        500g|           0|         0|    null|        null|https://shop.cole...|Great Ocean Road|      22.8|            1Kg|   SA|29935087|63253700|   2022-11-09|               11.4|\n",
      "|1010087P|BIGGS FLAT|192519|       5153|Dairy, eggs & fridge|      Cheese| Block cheese|Colby Cheese Block|$22.80 per 1Kg|        500g|           0|         0|    null|        null|https://shop.cole...|Great Ocean Road|      22.8|            1Kg|   SA|29935087|63253701|   2022-11-10| 11.424359008721346|\n",
      "|1010087P|BIGGS FLAT|192519|       5153|Dairy, eggs & fridge|      Cheese| Block cheese|Colby Cheese Block|$22.80 per 1Kg|        500g|           0|         0|    null|        null|https://shop.cole...|Great Ocean Road|      22.8|            1Kg|   SA|29935087|63253702|   2022-11-11| 11.392563537684705|\n",
      "|1010087P|BIGGS FLAT|192519|       5153|Dairy, eggs & fridge|      Cheese| Block cheese|Colby Cheese Block|$22.80 per 1Kg|        500g|           0|         0|    null|        null|https://shop.cole...|Great Ocean Road|      22.8|            1Kg|   SA|29935087|63253703|   2022-11-12| 11.389801096448968|\n",
      "|1010087P|BIGGS FLAT|192519|       5153|Dairy, eggs & fridge|      Cheese| Block cheese|Colby Cheese Block|$22.80 per 1Kg|        500g|           0|         0|    null|        null|https://shop.cole...|Great Ocean Road|      22.8|            1Kg|   SA|29935087|63253704|   2022-11-13| 11.429282506309084|\n",
      "+--------+----------+------+-----------+--------------------+------------+-------------+------------------+--------------+------------+------------+----------+--------+------------+--------------------+----------------+----------+---------------+-----+--------+--------+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select the RunDate and Package_price based on the availability in synthetic_df\n",
    "combined_df = combined_df.withColumn(\"RunDate_final\",\n",
    "    func.when(col(\"RunDate_synthetic\").isNotNull(), col(\"RunDate_synthetic\")).otherwise(col(\"RunDate_grocery\"))\n",
    "    ).withColumn(\"Package_price_final\",\n",
    "    func.when(col(\"Package_price_synthetic\").isNotNull(), col(\"Package_price_synthetic\")).otherwise(col(\"Package_price_grocery\"))\n",
    "    )\n",
    "\n",
    "# Drop intermediate columns if necessary\n",
    "combined_df = combined_df.drop(\"RunDate_grocery\", \"RunDate_synthetic\", \"Package_price_grocery\", \"Package_price_synthetic\")\n",
    "\n",
    "# Show a few rows of the final DataFrame to verify\n",
    "combined_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "310d296c-f415-4c28-a072-e831d7f82df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------------+\n",
      "|     Sku|RunDate_final|Package_price_final|\n",
      "+--------+-------------+-------------------+\n",
      "|1274566P|   2022-11-09|                3.5|\n",
      "|1274566P|   2022-11-10| 3.4992970916750075|\n",
      "|1274566P|   2022-11-11| 3.4919676439251823|\n",
      "|1274566P|   2022-11-12|  3.483318804164933|\n",
      "|1274566P|   2022-11-13|  3.476728436445554|\n",
      "|1274566P|   2022-11-14| 3.4713751650882436|\n",
      "|1274566P|   2022-11-15|  3.468289742285664|\n",
      "|1274566P|   2022-11-16|  3.469628802815525|\n",
      "|1274566P|   2022-11-17| 3.4685520919892463|\n",
      "|1274566P|   2022-11-18| 3.4618283238363503|\n",
      "|1274566P|   2022-11-19|  3.459560920240769|\n",
      "|1274566P|   2022-11-20| 3.4577208827467465|\n",
      "|1274566P|   2022-11-21| 3.4587357747089897|\n",
      "|1274566P|   2022-11-22| 3.4547550379785044|\n",
      "|1274566P|   2022-11-23| 3.4830236298370316|\n",
      "|1274566P|   2022-11-24|  3.465031927594927|\n",
      "|1274566P|   2022-11-25| 3.4631063215063502|\n",
      "|1274566P|   2022-11-26| 3.4582813749440575|\n",
      "|1274566P|   2022-11-27| 3.4847090121384356|\n",
      "|1274566P|   2022-11-28| 3.4807397217337352|\n",
      "+--------+-------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check data for SKU 1274566P before feature engineering\n",
    "combined_df.filter(col(\"Sku\") == \"1274566P\").select(\"Sku\", \"RunDate_final\", \"Package_price_final\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f57d382c-ab34-43ab-ba18-a5622639f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "necessary_columns = ['Sku', 'Category', 'Sub_category', 'Product_Group', 'Product_Name', 'Brand', 'state', 'city','RunDate_final', 'Package_price_final']\n",
    "\n",
    "combined_df = combined_df.select(necessary_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "063e17fc-163a-453a-9a09-a7d274a8271b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in combined_df: 189592320\n"
     ]
    }
   ],
   "source": [
    "# Get the count of rows in combined_df\n",
    "row_count = combined_df.count()\n",
    "\n",
    "# Print the count\n",
    "print(f\"Total number of rows in combined_df: {row_count}\") #189592320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cc4f41f-c1fb-48f8-ac3d-4bc8e8ccdddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of unique SKUs in combined_df\n",
    "#total_sku_count = combined_df.select(\"Sku\").distinct().count()\n",
    "\n",
    "#print(f\"Total number of unique SKUs: {total_sku_count}\")  -- 8092\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e38bfb85-5975-4e64-bf86-c10bf2a26919",
   "metadata": {},
   "outputs": [],
   "source": [
    "sku_counts = combined_df.groupBy(\"Sku\").count()\n",
    "\n",
    "# Step 2: Select the top 50 SKUs based on the count\n",
    "top_120_skus = sku_counts.orderBy(func.desc(\"count\")).limit(120).select(\"Sku\")\n",
    "\n",
    "# Step 3: Collect top 50 SKUs as a list\n",
    "top_120_sku_list = [row['Sku'] for row in top_120_skus.collect()]\n",
    "\n",
    "# Step 4: Filter the combined_df to keep only the rows with SKUs in top_120_sku_list\n",
    "combined_df = combined_df.filter(combined_df.Sku.isin(top_120_sku_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89ba5805-bfd5-4f84-91c1-66b4a4a0d3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows in combined_df: 7726244\n"
     ]
    }
   ],
   "source": [
    "# Get the count of rows in combined_df\n",
    "row_count = combined_df.count()\n",
    "\n",
    "# Print the count\n",
    "print(f\"Total number of rows in combined_df: {row_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "315cdf60-ac0c-4de7-8384-c887617bca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.withColumn(\"RunDate_final_str\", func.date_format(\"RunDate_final\", \"yyyy-MM-dd\"))\n",
    "\n",
    "# Apply Label Encoding to categorical columns\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=column, outputCol=column+'_index').fit(combined_df)\n",
    "    for column in ['Category', 'Sub_category', 'Product_Group', 'Brand', 'state', 'city']\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "combined_df = pipeline.fit(combined_df).transform(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1b951df-2f60-4206-870d-5576fb384135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Sku: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Sub_category: string (nullable = true)\n",
      " |-- Product_Group: string (nullable = true)\n",
      " |-- Product_Name: string (nullable = true)\n",
      " |-- Brand: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- RunDate_final: string (nullable = true)\n",
      " |-- Package_price_final: double (nullable = true)\n",
      " |-- RunDate_final_str: string (nullable = true)\n",
      " |-- Category_index: double (nullable = false)\n",
      " |-- Sub_category_index: double (nullable = false)\n",
      " |-- Product_Group_index: double (nullable = false)\n",
      " |-- Brand_index: double (nullable = false)\n",
      " |-- state_index: double (nullable = false)\n",
      " |-- city_index: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e18d2f01-c633-4c7b-a85a-59a9f14620d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package_price_synthetic: 0 missing values after filling with median\n"
     ]
    }
   ],
   "source": [
    "median_prices = combined_df.groupBy(\"Sku\", \"city\").agg(func.expr('percentile_approx(Package_price_final, 0.5)').alias('median_price'))\n",
    "combined_df = combined_df.join(median_prices, on=[\"Sku\", \"city\"], how=\"left\")\n",
    "combined_df = combined_df.withColumn(\"Package_price_final\",func.when(func.col(\"Package_price_final\").isNull(), func.col(\"median_price\")).otherwise(func.col(\"Package_price_final\"))).drop(\"median_price\")\n",
    "missing_count_price = combined_df.filter(func.col(\"Package_price_final\").isNull()).count()\n",
    "print(f\"Package_price_synthetic: {missing_count_price} missing values after filling with median\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5341ad3f-d1b9-4a0e-a371-ae3eec93149c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------------------+\n",
      "|     Sku|RunDate_final|Package_price_final|\n",
      "+--------+-------------+-------------------+\n",
      "|1274566P|   2022-11-09|                3.5|\n",
      "|1274566P|   2022-11-10| 3.4992970916750075|\n",
      "|1274566P|   2022-11-11| 3.4919676439251823|\n",
      "|1274566P|   2022-11-12|  3.483318804164933|\n",
      "|1274566P|   2022-11-13|  3.476728436445554|\n",
      "|1274566P|   2022-11-14| 3.4713751650882436|\n",
      "|1274566P|   2022-11-15|  3.468289742285664|\n",
      "|1274566P|   2022-11-16|  3.469628802815525|\n",
      "|1274566P|   2022-11-17| 3.4685520919892463|\n",
      "|1274566P|   2022-11-18| 3.4618283238363503|\n",
      "|1274566P|   2022-11-19|  3.459560920240769|\n",
      "|1274566P|   2022-11-20| 3.4577208827467465|\n",
      "|1274566P|   2022-11-21| 3.4587357747089897|\n",
      "|1274566P|   2022-11-22| 3.4547550379785044|\n",
      "|1274566P|   2022-11-23| 3.4830236298370316|\n",
      "|1274566P|   2022-11-24|  3.465031927594927|\n",
      "|1274566P|   2022-11-25| 3.4631063215063502|\n",
      "|1274566P|   2022-11-26| 3.4582813749440575|\n",
      "|1274566P|   2022-11-27| 3.4847090121384356|\n",
      "|1274566P|   2022-11-28| 3.4807397217337352|\n",
      "+--------+-------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check data for SKU 1274566P before feature engineering\n",
    "combined_df.filter(col(\"Sku\") == \"1274566P\").select(\"Sku\", \"RunDate_final\", \"Package_price_final\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8f3ebe-ea6f-4224-b639-dfff4d950074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sku: 0 missing values\n",
      "city: 0 missing values\n",
      "Category: 0 missing values\n",
      "Sub_category: 0 missing values\n",
      "Product_Group: 0 missing values\n",
      "Product_Name: 0 missing values\n",
      "Brand: 0 missing values\n"
     ]
    }
   ],
   "source": [
    "#median_prices = combined_df.groupBy(\"Sku\", \"city\").agg(func.expr('percentile_approx(Package_price_synthetic, 0.5)').alias('median_price'))\n",
    "#combined_df = combined_df.join(median_prices, on=[\"Sku\", \"city\"], how=\"left\")\n",
    "#combined_df = combined_df.withColumn(\"Package_price_synthetic\",func.when(func.col(\"Package_price_synthetic\").isNull(), func.col(\"median_price\")).otherwise(func.col(\"Package_price_synthetic\"))).drop(\"median_price\")\n",
    "#missing_count_price = combined_df.filter(F.col(\"Package_price_synthetic\").isNull()).count()\n",
    "#print(f\"Package_price_synthetic: {missing_count_price} missing values after filling with median\")\n",
    "\n",
    "#median_prices = combined_df.groupBy(\"Sku\", \"city\").agg(func.expr('percentile_approx(Package_price_synthetic, 0.5)').alias('median_price'))\n",
    "#combined_df = combined_df.join(median_prices, on=[\"Sku\", \"city\"], how=\"left\")\n",
    "#combined_df = combined_df.withColumn(\"Package_price_synthetic\",func.when(func.col(\"Package_price_synthetic\").isNull(), func.col(\"median_price\")).otherwise(F.col(\"Package_price_synthetic\"))).drop(\"median_price\")\n",
    "        \n",
    "for column in combined_df.columns:\n",
    "    # Check if the column is of type FloatType or DoubleType\n",
    "    if column == 'Package_price_final':\n",
    "        pass       \n",
    "    elif dict(combined_df.dtypes)[column] in ['float', 'double','integer' ]:\n",
    "        # Check for both NaN and Null values\n",
    "        missing_count = combined_df.filter(func.col(column).isNull() | func.isnan(column)).count()\n",
    "        combined_df = combined_df.withColumn(column, func.when(func.col(column).isNull() | func.isnan(column), 0.0).otherwise(func.col(column)))\n",
    "\n",
    "    \n",
    "    elif dict(combined_df.dtypes)[column] in ['boolean']:\n",
    "        missing_count = combined_df.filter(func.col(column).isNull()).count()\n",
    "        combined_df = combined_df.withColumn(column, func.when(func.col(column).isNull(), False).otherwise(func.col(column)))\n",
    "        \n",
    "    else:\n",
    "        # Only check for Null values\n",
    "        missing_count = combined_df.filter(func.col(column).isNull()).count()\n",
    "        combined_df = combined_df.withColumn(column, func.when(func.col(column).isNull(), '').otherwise(func.col(column)))\n",
    "    \n",
    "    print(f\"{column}: {missing_count} missing values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78713d1-d889-4b6b-af79-8b76a9304f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81800f3c-3343-4974-a8eb-85689a56548b",
   "metadata": {},
   "source": [
    "# Creating Lag Features (1 day, 7 days, 30 days, 60 days, 90 days)\n",
    "windowSpec = Window.partitionBy(\"Sku\", \"city\").orderBy(\"RunDate_final\")\n",
    "combined_df = combined_df.withColumn(\"lag_1\", func.lag(\"Package_price_final\", 1).over(windowSpec))\n",
    "combined_df = combined_df.withColumn(\"lag_7\", func.lag(\"Package_price_final\", 7).over(windowSpec))\n",
    "combined_df = combined_df.withColumn(\"lag_30\", func.lag(\"Package_price_final\", 30).over(windowSpec))\n",
    "combined_df = combined_df.withColumn(\"lag_60\", func.lag(\"Package_price_final\", 60).over(windowSpec))\n",
    "combined_df = combined_df.withColumn(\"lag_90\", func.lag(\"Package_price_final\", 90).over(windowSpec))\n",
    "\n",
    "# Creating Rolling Statistics using `RunDate_synthetic`\n",
    "combined_df = combined_df.withColumn(\"rolling_mean_7\", func.avg(\"Package_price_final\").over(windowSpec.rowsBetween(-6, 0)))\n",
    "combined_df = combined_df.withColumn(\"rolling_mean_14\", func.avg(\"Package_price_final\").over(windowSpec.rowsBetween(-13, 0)))\n",
    "combined_df = combined_df.withColumn(\"rolling_mean_30\", func.avg(\"Package_price_final\").over(windowSpec.rowsBetween(-29, 0)))\n",
    "\n",
    "# Creating Rolling Standard Deviation (Price Volatility)\n",
    "combined_df = combined_df.withColumn(\"rolling_std_7\", func.stddev(\"Package_price_final\").over(windowSpec.rowsBetween(-6, 0)))\n",
    "combined_df = combined_df.withColumn(\"rolling_std_30\", func.stddev(\"Package_price_final\").over(windowSpec.rowsBetween(-29, 0)))\n",
    "\n",
    "# Creating Price Differencing Feature\n",
    "combined_df = combined_df.withColumn(\"price_diff\", combined_df[\"Package_price_final\"] - func.lag(combined_df[\"Package_price_final\"], 1).over(windowSpec))\n",
    "\n",
    "# Seasonality Features using `RunDate_synthetic`\n",
    "combined_df = combined_df.withColumn(\"day_of_week\", func.dayofweek(\"RunDate_final\"))\n",
    "combined_df = combined_df.withColumn(\"month\", func.month(\"RunDate_final\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94af3635-1fe8-4397-9237-46f77524adb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Lag Features (1 day, 7 days, 30 days, 60 days, 90 days)\n",
    "windowSpec = Window.partitionBy(\"Sku\", \"city\").orderBy(\"RunDate_final\").rowsBetween(Window.unboundedPreceding, -1)\n",
    "combined_df = combined_df.withColumn(\"lag_1\", func.lag(\"Package_price_final\", 1).over(windowSpec))\n",
    "combined_df = combined_df.withColumn(\"lag_7\", func.lag(\"Package_price_final\", 7).over(windowSpec))\n",
    "combined_df = combined_df.withColumn(\"lag_30\", func.lag(\"Package_price_final\", 30).over(windowSpec))\n",
    "combined_df = combined_df.withColumn(\"lag_60\", func.lag(\"Package_price_final\", 60).over(windowSpec))\n",
    "combined_df = combined_df.withColumn(\"lag_90\", func.lag(\"Package_price_final\", 90).over(windowSpec))\n",
    "\n",
    "# Creating Rolling Statistics using `RunDate_synthetic`\n",
    "combined_df = combined_df.withColumn(\"rolling_mean_7\", func.avg(\"Package_price_final\").over(windowSpec))\n",
    "combined_df = combined_df.withColumn(\"rolling_mean_14\", func.avg(\"Package_price_final\").over(windowSpec))\n",
    "combined_df = combined_df.withColumn(\"rolling_mean_30\", func.avg(\"Package_price_final\").over(windowSpec))\n",
    "\n",
    "# Creating Rolling Standard Deviation (Price Volatility)\n",
    "combined_df = combined_df.withColumn(\"rolling_std_7\", func.stddev(\"Package_price_final\").over(windowSpec))\n",
    "combined_df = combined_df.withColumn(\"rolling_std_30\", func.stddev(\"Package_price_final\").over(windowSpec))\n",
    "\n",
    "# Creating Price Differencing Feature\n",
    "combined_df = combined_df.withColumn(\"price_diff\", combined_df[\"Package_price_final\"] - func.lag(combined_df[\"Package_price_final\"], 1).over(windowSpec))\n",
    "\n",
    "# Seasonality Features using `RunDate_synthetic`\n",
    "combined_df = combined_df.withColumn(\"day_of_week\", func.dayofweek(\"RunDate_final\"))\n",
    "combined_df = combined_df.withColumn(\"month\", func.month(\"RunDate_final\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634c543e-10db-4585-9fb3-6e3214033ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\n",
    "    \"lag_1\", \"lag_7\", \"lag_30\", \"lag_60\", \"lag_90\",\n",
    "    \"rolling_mean_7\", \"rolling_mean_14\", \"rolling_mean_30\",\n",
    "    \"rolling_std_7\", \"rolling_std_30\", \"price_diff\",\n",
    "    \"day_of_week\", \"month\",\n",
    "    \"Category_index\", \"Sub_category_index\",\n",
    "    \"Product_Group_index\", \n",
    "    \"Brand_index\", \"state_index\", \"city_index\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da23cc16-0971-4008-9318-56044c2e9c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in feature_columns:\n",
    "    missing_count = combined_df.filter(func.col(column).isNull()).count()\n",
    "    if missing_count > 0:\n",
    "        print(f\"Column '{column}' has {missing_count} missing values. Filling them with zeros.\")\n",
    "        combined_df = combined_df.withColumn(column, func.when(func.col(column).isNull(), 0.0).otherwise(func.col(column)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334099e3-350a-40f1-80db-56227b036335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "\n",
    "if 'features' in combined_df.columns:\n",
    "    combined_df = combined_df.drop('features')\n",
    "    \n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features', handleInvalid='skip')\n",
    "combined_df = assembler.transform(combined_df)\n",
    "\n",
    "# Apply StandardScaler to scale the 'features' column\n",
    "#scaler = StandardScaler(inputCol='features', outputCol='scaled_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f4b046-a65a-4a92-8eda-8dc115501084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaled_features', withMean=True, withStd=True)\n",
    "#scaler = StandardScaler().setInputCol('features').setOutputCol('scaled_features')\n",
    "scaler_model = scaler.fit(combined_df)\n",
    "scaled_df = scaler_model.transform(combined_df)\n",
    "end_time = time.time()\n",
    "elapsed_time = start_time - end_time\n",
    "print(f'Time Elapsed : {elapsed_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139f6b1c-ec1a-48fb-8154-901f7e5a6d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of records in scaled_df: {scaled_df.count()}\")\n",
    "scaled_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4477b15d-2dfc-4d40-8815-6f6f9e670251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas for further processing with XGBoost and Exponential Smoothing\n",
    "pandas_df = scaled_df.select(\"scaled_features\", \"Sku\", \"Category\", \"Sub_category\", \"Product_Group\", \"Product_Name\", \"Brand\", \"state\", \"city\", \"Package_price_final\", \"RunDate_final\").toPandas()\n",
    "\n",
    "cutoff_date = '2023-11-01'\n",
    "\n",
    "# Split into train and test sets\n",
    "train_df = pandas_df[pandas_df['RunDate_final'] < cutoff_date]\n",
    "test_df = pandas_df[pandas_df['RunDate_final'] >= cutoff_date]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b9ec9a-3ad2-4ba5-a068-cca82487f653",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df['scaled_features'].tolist()\n",
    "y_train = train_df['Package_price_final']\n",
    "X_test = test_df['scaled_features'].tolist()\n",
    "y_test = test_df['Package_price_final']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b496b5-6879-4468-a5d1-11d7106f9409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: XGBoost\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "xgb_model = XGBRegressor(objective='reg:squarederror')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b7d2e4-c90f-4a7e-96c6-f0cfe44638c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate XGBoost Model\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost MSE: {mse_xgb}\")     #XGBoost MSE: 0.02099907390528608\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be518ed5-23fd-450d-847c-17daae5004d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE for XGBoost\n",
    "rmse_xgb = np.sqrt(mse_xgb)\n",
    "print(f\"XGBoost RMSE: {rmse_xgb}\")   #XGBoost RMSE: 0.14491057209633146"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab779b4-40e1-40d6-aab3-26c13dba28f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAE for XGBoost\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost MAE: {mae_xgb}\")    #XGBoost MAE: 0.0392099127999695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22ab1ad-c517-4f16-a95f-bbfd6bfce3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate R-squared for XGBoost\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost R-Squared: {r2_xgb}\")    #XGBoost R-Squared: 0.9995470814112523\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6c7499-990f-4c3b-a892-39155aad588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the prices - pred vs actual\n",
    "test_df_pd = test_df.copy()\n",
    "\n",
    "# Add the actual and predicted values to the DataFrame\n",
    "test_df_pd['Actual'] = y_test\n",
    "test_df_pd['Predicted_XGBoost'] = y_pred_xgb\n",
    "#test_df_pd['Predicted_ExponentialSmoothing'] = y_pred_es[:len(y_test)]  # Ensure the length matches y_test\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "test_df_pd.to_csv('predictions_with_all_features1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325acaca-101b-4142-812a-0c275e7c1bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Exponential Smoothing\n",
    "es_model = ExponentialSmoothing(y_train, trend=\"add\", seasonal=\"add\", seasonal_periods=12).fit()  \n",
    "y_pred_es = es_model.forecast(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6882e2e8-b500-4c1a-a11c-d2a5db20549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Exponential Smoothing Model\n",
    "mse_es = mean_squared_error(y_test, y_pred_es)\n",
    "print(f\"Exponential Smoothing MSE: {mse_es}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
